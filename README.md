# ğŸ¤– Viral Content Creator AI

Este proyecto es un pipeline automatizado de generaciÃ³n de contenido de video, diseÃ±ado para transformar una simple idea en una serie de videos cortos (estilo Reels/Shorts) inmersivos y listos para publicar en redes sociales. Utiliza una arquitectura modular basada en agentes de IA para orquestar el proceso completo, desde la creaciÃ³n del guion hasta la generaciÃ³n de los recursos multimedia.

## âœ¨ CaracterÃ­sticas Principales

- **GeneraciÃ³n de Guiones con IA:** A partir de una idea inicial, un LLM (GPT-4o) genera una estructura de guion completa, incluyendo una narrativa secuencial de escenas, una descripciÃ³n del entorno y hashtags relevantes.
- **Prompts Optimizados:** El sistema transforma las descripciones de las escenas en prompts altamente optimizados (basados en palabras clave) para modelos de texto-a-imagen, mejorando drÃ¡sticamente la calidad y coherencia de las imÃ¡genes.
- **GeneraciÃ³n de ImÃ¡genes POV:** Utiliza modelos de difusiÃ³n (como Stable Diffusion a travÃ©s de la API de Hugging Face) para crear imÃ¡genes fotorrealistas y cinematogrÃ¡ficas desde una perspectiva en primera persona (POV).
- **OrquestaciÃ³n Modular con LangGraph:** Todo el flujo de trabajo estÃ¡ gestionado por un grafo de estados (Stateful Graph) implementado con LangGraph, lo que permite una gran flexibilidad, modularidad y un manejo de errores robusto.
- **Arquitectura Escalable:** El cÃ³digo estÃ¡ organizado en mÃ³dulos lÃ³gicos (generaciÃ³n de contenido, multimedia, ediciÃ³n de video, publicaciÃ³n), facilitando la adiciÃ³n de nuevas funcionalidades o el cambio de APIs (ej. cambiar de DALL-E a Stable Diffusion).

## ğŸ› ï¸ Stack TecnolÃ³gico

- **OrquestaciÃ³n y Agentes IA:** LangChain, LangGraph
- **Modelos de Lenguaje (LLM):** OpenAI (GPT-4o)
- **GeneraciÃ³n de ImÃ¡genes:** Hugging Face Inference API (Stable Diffusion)
- **EdiciÃ³n de Video (futuro):** MoviePy
- **GestiÃ³n de Dependencias:** Pip (ver `requirements.txt`)
- **Base de Datos (futuro):** SQLAlchemy con SQLite

## ğŸ“‚ Estructura del Proyecto

```
content-creator-2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/             # Define el grafo de LangGraph y los nodos del flujo
â”‚   â”‚   â””â”€â”€ graph.py
â”‚   â”œâ”€â”€ logic/              # Contiene la lÃ³gica de negocio principal
â”‚   â”‚   â”œâ”€â”€ content_generator.py  # GeneraciÃ³n de guiones y prompts
â”‚   â”‚   â”œâ”€â”€ multimedia_generator.py # GeneraciÃ³n de imÃ¡genes y audio
â”‚   â”‚   â”œâ”€â”€ video_editor.py       # EdiciÃ³n y composiciÃ³n de video
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ data/               # Modelos de base de datos (SQLAlchemy)
â”œâ”€â”€ assets/                 # Directorio para los recursos generados (imÃ¡genes, videos)
â”œâ”€â”€ .env                    # Archivo para variables de entorno (API keys)
â”œâ”€â”€ main.py                 # Punto de entrada de la aplicaciÃ³n
â””â”€â”€ requirements.txt        # Dependencias de Python
```

## ğŸš€ CÃ³mo Empezar

### Prerrequisitos

- Python 3.9+
- Una cuenta de OpenAI y una API key.
- Una cuenta de Hugging Face y un API token.

### InstalaciÃ³n

1.  **Clona el repositorio:**
    ```bash
    git clone <URL_DEL_REPOSITORIO>
    cd content-creator-2
    ```

2.  **Crea y activa un entorno virtual (recomendado):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    ```

3.  **Instala las dependencias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configura las variables de entorno:**
    Crea un archivo `.env` en la raÃ­z del proyecto y aÃ±ade tus claves de API:
    ```env
    OPENAI_API_KEY="tu_api_key_de_openai"
    HF_TOKEN="tu_api_token_de_hugging_face"
    NUM_SCENES=5
    ```

### EjecuciÃ³n

Para iniciar el pipeline, ejecuta el script principal:

```bash
python main.py
```

El programa te pedirÃ¡ que introduzcas una idea para el video, y comenzarÃ¡ el proceso de generaciÃ³n de contenido. Los resultados se guardarÃ¡n en la carpeta `assets/`.

## ğŸ”® Futuro del Proyecto

- **GeneraciÃ³n de Video por IA:** Integrar un modelo de Image-to-Video (ej. Stable Video Diffusion) para convertir las imÃ¡genes estÃ¡ticas en clips de video dinÃ¡micos.
- **Sonido Ambiental:** Implementar la generaciÃ³n de audio a partir de prompts usando APIs como la de ElevenLabs o Stability AI.
- **PublicaciÃ³n AutomÃ¡tica:** Desarrollar los mÃ³dulos de `social_publisher` para subir el contenido final directamente a plataformas como YouTube, TikTok e Instagram.
